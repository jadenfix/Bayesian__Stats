{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 1: Jaden Fix and Matteo Shafer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in packages\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import hypergeom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used GPT to sharpen up my code and to combat a few syntax errors\n",
    "I also code with co-pilot enabled on my IDE.\n",
    "Didn't discuss with any other students except one another (and Aditya) before turning this in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "Given a binomial experiment with 400 trials, where x = 216 successes are observed, and H0: q = 0.5:\n",
    "\n",
    "a. Estimates the probability of success.\n",
    "\n",
    "b. Calculates a two‐tailed p-value (summing tail probabilities for outcomes as extreme or more extreme than x = 216).\n",
    "\n",
    "c. Computes likelihood ratios comparing the probability of observing 216 successes under three alternative hypotheses (q = 0.56, 0.54, 0.52) relative to H0.\n",
    "\n",
    "d. Prints a brief discussion of the results.\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem 1 A.)\n",
      "\n",
      "Estimated probability of success (q̂): 0.54\n",
      "\n",
      "Problem 1 B.)\n",
      "\n",
      "Two-tailed p-value: 0.12103283684609918\n",
      "\n",
      "Problem 1 C.)\n",
      "\n",
      "Likelihood ratios for alternative q values:\n",
      "  For q = 0.56: Likelihood Ratio = 2.606\n",
      "  For q = 0.54: Likelihood Ratio = 3.602\n",
      "  For q = 0.52: Likelihood Ratio = 2.613\n",
      "\n",
      "Problem 1 D.)\n",
      "\n",
      "P-value from part (b): 0.1210\n",
      "This suggests that the observed result is consistent with the null hypothesis.\n",
      "q = 0.56: Likelihood Ratio = 2.6056\n",
      "q = 0.54: Likelihood Ratio = 3.6016\n",
      "q = 0.52: Likelihood Ratio = 2.6128\n",
      "\n",
      "The highest likelihood ratio is for q = 0.54.\n",
      "This means the data is most compatible with q = 0.54, making the evidence strongest against the null in this case.\n"
     ]
    }
   ],
   "source": [
    "# Problem 1 A.)\n",
    "print(\"\\nProblem 1 A.)\\n\")\n",
    "n = 400  # total number of trials\n",
    "x = 216  # number of successes\n",
    "q_null = 0.5  # null hypothesis probability of success\n",
    "q_hat = x / n\n",
    "print(\"Estimated probability of success (q̂):\", q_hat)\n",
    "\n",
    "# Problem 1 B.)\n",
    "print(\"\\nProblem 1 B.)\\n\")\n",
    "p_lower = binom.cdf(184, n, q_null)\n",
    "p_upper = binom.sf(215, n, q_null)\n",
    "p_value = p_lower + p_upper\n",
    "print(\"Two-tailed p-value:\", p_value)\n",
    "\n",
    "# Problem 1 C.)\n",
    "print(\"\\nProblem 1 C.)\\n\")\n",
    "alts = [0.56, 0.54, 0.52]\n",
    "print(\"Likelihood ratios for alternative q values:\")\n",
    "for q_alt in alts:\n",
    "    likelihood_alt = binom.pmf(x, n, q_alt)\n",
    "    likelihood_null = binom.pmf(x, n, q_null)\n",
    "    lr = likelihood_alt / likelihood_null\n",
    "    print(f\"  For q = {q_alt}: Likelihood Ratio = {lr:.4g}\")\n",
    "\n",
    "# Problem 1 D.)\n",
    "print(\"\\nProblem 1 D.)\\n\")\n",
    "print(f\"P-value from part (b): {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"This suggests that the observed result is somewhat unlikely under the null hypothesis (q = 0.5).\")\n",
    "else:\n",
    "    print(\"This suggests that the observed result is consistent with the null hypothesis.\")\n",
    "\n",
    "likelihoods = []\n",
    "for q_alt in alts:\n",
    "    L_alt = binom.pmf(x, n, q_alt)\n",
    "    L_null = binom.pmf(x, n, q_null)\n",
    "    lr = L_alt / L_null\n",
    "    likelihoods.append((q_alt, lr))\n",
    "    print(f\"q = {q_alt}: Likelihood Ratio = {lr:.4f}\")\n",
    "\n",
    "best_alt, best_lr = max(likelihoods, key=lambda t: t[1])\n",
    "print(f\"\\nThe highest likelihood ratio is for q = {best_alt}.\")\n",
    "print(f\"This means the data is most compatible with q = {best_alt}, making the evidence strongest against the null in this case.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:\n",
    "In a normally distributed setting (x̄ ~ Normal(μ, σ/√n)), it:\n",
    "\n",
    "1. Identifies the effect size most “compatible” with the data (i.e. where the two-sided p-value is maximized; note that for a two‐sided test the maximum p‐value is achieved at μ = d).\n",
    "\n",
    "2. Computes the set (a 95% confidence interval) of effect sizes μ for which the two‐sided p-value exceeds 0.05. (You are prompted for an observed mean, known σ, and sample size n.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error (σ/√n) = 0.3651483716701107\n",
      "\n",
      "Most compatible effect size (μ*): 10.0\n",
      "p-value at μ* = 1.0\n",
      "\n",
      "95% confidence interval for μ (p > 0.05): (9.284, 10.716)\n",
      "This means that since the true mean is within this interval, we would not reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nProblem 2:\\n\")\n",
    "d = 10.0      # observed sample mean\n",
    "sigma = 2.0   # known standard deviation\n",
    "n = 30        # number of observations\n",
    "se = sigma / math.sqrt(n)\n",
    "print(\"Standard error (σ/√n) =\", se)\n",
    "\n",
    "\n",
    "def p_value(mu, d, se):\n",
    "    T = abs(d - mu) / se  # test statistic\n",
    "    return 2 * (1 - norm.cdf(T))\n",
    "\n",
    "mu_star = d\n",
    "p_star = p_value(mu_star, d, se)\n",
    "print(\"\\nMost compatible effect size (μ*):\", mu_star)\n",
    "print(\"p-value at μ* =\", p_star)  # should be 1.0\n",
    "\n",
    "# For a two-sided test, p > 0.05 when |d - μ| < z_crit * se.\n",
    "z_crit = norm.ppf(0.975)  # ≈ 1.96\n",
    "lower_bound = d - z_crit * se\n",
    "upper_bound = d + z_crit * se\n",
    "#round to 3 decimal places\n",
    "\n",
    "print(\"\\n95% confidence interval for μ (p > 0.05):\", (round(lower_bound,3), round(upper_bound,3)))\n",
    "print(\"This means that since the true mean is within this interval, we would not reject the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "Uses the hypergeometric distribution to compute the p-value for testing whether the incidents that occurred during Lucia’s shifts (observed count provided by the user) are unusually high relative to what is expected under the null hypothesis. (The script will ask you for the total number of nurse shifts (N), the number of shifts Lucia worked (K), and the total number of incidents observed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergeometric p-value (pooled data): 3.7015685432150337e-07\n",
      "Reject H0 at significance level 0.0001\n"
     ]
    }
   ],
   "source": [
    "N = 1704         # total number of nurse shifts (in the pooled data)\n",
    "K = 203           # number of shifts worked by Lucia\n",
    "n_incidents = 27 # total number of incidents observed\n",
    "observed = 14    # number of incidents during Lucia's shifts\n",
    "# hypergeom.sf(observed - 1, N, K, n_incidents) gives P(X ≥ observed)\n",
    "p_val = hypergeom.sf(observed - 1, N, K, n_incidents)\n",
    "print(\"Hypergeometric p-value (pooled data):\", p_val)\n",
    "alpha = 1 / 10000  # significance level (1 in 10000)\n",
    "if p_val < alpha:\n",
    "    print(f\"Reject H0 at significance level {alpha}\")\n",
    "else:\n",
    "    print(f\"Do not reject H0 at significance level {alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:\n",
    "Repeats the analysis of Problem 3 but for a restricted data set (for example, only for the RKZ hospitals). The p-value is computed in the same way with parameters that you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergeometric p-value (restricted data): 0.004546013865499521\n",
      "Do not reject H0 at significance level 0.0001\n",
      "The old p-value 3.7015685432150337e-07 is less than the new one: 0.004546013865499521.\n",
      "We reject the null hypothesis in the pooled data set, but not in the restricted data set.\n"
     ]
    }
   ],
   "source": [
    "N_res = 675       # total nurse shifts in the restricted data set\n",
    "K_res = 61        # shifts worked by Lucia in the restricted set\n",
    "n_res = 19        # total number of incidents in the restricted set\n",
    "observed_res = 6 # incidents during Lucia's shifts (restricted)\n",
    "p_val_res = hypergeom.sf(observed_res - 1, N_res, K_res, n_res)\n",
    "print(\"Hypergeometric p-value (restricted data):\", p_val_res)\n",
    "alpha = 1 / 10000\n",
    "if p_val_res < alpha:\n",
    "    print(f\"Reject H0 at significance level {alpha}\")\n",
    "else:\n",
    "    print(f\"Do not reject H0 at significance level {alpha}\")\n",
    "\n",
    "print(f\"The old p-value {p_val} is less than the new one: {p_val_res}.\")\n",
    "print(f\"We reject the null hypothesis in the pooled data set, but not in the restricted data set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5:\n",
    "Discussion on post-hoc analysis: what it is, why it is problematic, and how it may favor finding Lucia guilty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "\n",
    "Post-hoc analysis involves forming and testing a hypothesis using the same data that suggested the hypothesis, rather than specifying it in advance.\n",
    "\n",
    "•\tKey Issues:\n",
    "\n",
    "•\tInflated Type I Error: Increases the risk of false positives due to multiple testing.\n",
    "\n",
    "•\tBias & Data Dredging: The hypothesis is influenced by the data, leading to overfitting and overinterpretation of random patterns.\n",
    "\n",
    "•\tImplications for Lucia’s Case:\n",
    "\n",
    "Because the initial suspicion (more incidents during Lucia’s shifts) was generated from the data, any subsequent test using that data is biased. This can falsely strengthen the evidence against the null hypothesis, favoring a guilty verdict.\n",
    "\n",
    "•\tConclusion:\n",
    "\n",
    "To ensure valid inference, hypotheses should be pre-specified, not derived post-hoc from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECON_544_S24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
